#+TITLE: Using & testing MFLike likelihood
#+PROPERTY: header-args:jupyter-python :session mflike_tutorial
#+PROPERTY: header-args :exports both
#+PROPERTY: header-args :tangle mflike_tutorial.py

This notebook provides some examples on how to interact programmatically with the =MFLike= likelihood.

* Emacs config                                                     :noexport:
#+BEGIN_SRC elisp :session mflike_tutorial :results none :tangle non
  (setenv "WORKON_HOME" (concat (getenv "HOME") "/Workdir/CMB/development/LAT_MFLike"))
  (pyvenv-workon "pyenv")
#+END_SRC

* Preamble
#+BEGIN_SRC jupyter-python
  %matplotlib inline
  import numpy as np
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  import cobaya
  import camb
  print("     Numpy :", np.__version__)
  print("Matplotlib :", mpl.__version__)
  print("      CAMB :", camb.__version__)
  print("    Cobaya :", cobaya.__version__)
#+END_SRC

#+RESULTS:
:      Numpy : 1.18.1
: Matplotlib : 3.1.2
:       CAMB : 1.1.0
:     Cobaya : 2.1.0

* Loading & getting the MFLike likelihood from cobaya

We first set the default values of CMB and nuisance parameters
#+BEGIN_SRC jupyter-python :results none
  cosmo_params = {
      "cosmomc_theta": 0.0104085,
      "logA": {"value": 3.044, "drop": True},
      "As": {"value": "lambda logA: 1e-10*np.exp(logA)"},
      "ombh2": 0.02237,
      "omch2": 0.1200,
      "ns": 0.9649,
      "Alens": 1.0,
      "tau": 0.0544
  }
  nuisance_params = {
      "a_tSZ": 3.30,
      "a_kSZ": 1.60,
      "a_p": 6.90,
      "beta_p": 2.08,
      "a_c": 4.90,
      "beta_c": 2.20,
      "n_CIBC": 1.20,
      "a_s": 3.10,
      "T_d": 9.60
  }
#+END_SRC

Then we declare our likelihood
#+BEGIN_SRC jupyter-python :results none
  mflike_config = {
      "mflike.MFLike": {
          "input_file": "data_sacc_00000.fits",
          "cov_Bbl_file": "data_sacc_w_covar_and_Bbl.fits"
      }
  }
#+END_SRC

We will need to download and install [[https://github.com/simonsobs/LAT_MFLike_data][=LAT_MFLike_data=]]. Here we will do it programatically in such a
way we can use this notebook without any prerequisites. Code and data will be stored in the =/tmp=
directory for the tutorial purpose but feel free to change it to whatever location you want. By the
way the next command will take some times to proceed
#+BEGIN_SRC jupyter-python
  from cobaya.install import install
  install({"likelihood": mflike_config}, path="/tmp/modules")
#+END_SRC

#+RESULTS:
: [install] Installing modules at '/tmp/modules'
:
: ================================================================================
: likelihood:mflike.MFLike
: ================================================================================
:
: [install] External module already installed.
:
: [install] Doing nothing.
:

We finally put everything into a dictionary to push it into =cobaya= configuration system
#+BEGIN_SRC jupyter-python :results none
  info = {
      "params": {**cosmo_params, **nuisance_params},
      "likelihood": mflike_config,
      "theory": {"camb": {"extra_args": {"lens_potential_accuracy": 1}}},
      "modules": "/tmp/modules"
  }
#+END_SRC

We will now get the model given the above parameters
#+BEGIN_SRC jupyter-python
  from cobaya.model import get_model
  model = get_model(info)
#+END_SRC

#+RESULTS:
: [prior] *WARNING* No sampled parameters requested! This will fail for non-mock samplers.
: [camb] *local* CAMB not found at /tmp/modules/code/CAMB
: [camb] Importing *global* CAMB.
: [mflike.mflike] Initialising.

To get a direct acces to the =MFLike= likelihood instance, we can retrieve it from the collection of
likelihoods of =cobaya= given its name (so far there is only our likelihood)
#+BEGIN_SRC jupyter-python :results none
  mflike = model.likelihood["mflike.MFLike"]
#+END_SRC

We can now play with the different data (Bbl, inverted covariance matrix) and we can also call the
different methods such as =_get_foreground_model= to see what foreground model look likes.

* Getting likelihood value given current parameter set

We can retrieve informations related to likelihood(s) as follow
#+BEGIN_SRC jupyter-python
  loglikes, derived = model.loglikes({})
  print("log-likelihood value = {}, derived parameter value (As) = {}".format(loglikes, derived))
#+END_SRC

#+RESULTS:
: log-likelihood value = [-1533.26635245], derived parameter value (As) = [2.0989031673191437e-09]

We can also use the =evaluate= sampler that evaluates the log-likelihood at a given reference point :
here, since parameters are all fixed, it computes the log-likelihood value.
#+BEGIN_SRC jupyter-python :async yes
  info["sampler"] = {"evaluate": None}
  from cobaya.run import run
  updated_info, products = run(info)
#+END_SRC

#+RESULTS:
#+begin_example
  [prior] *WARNING* No sampled parameters requested! This will fail for non-mock samplers.
  [camb] *local* CAMB not found at /tmp/modules/code/CAMB
  [camb] Importing *global* CAMB.
  [mflike.mflike] Initialising.
  [evaluate] Initialized!
  [evaluate] Looking for a reference point with non-zero prior.
  [evaluate] Reference point:

  [evaluate] Evaluating prior and likelihoods...
  [evaluate] log-posterior  = -1533.27
  [evaluate] log-prior      = 0
  [evaluate]    logprior_0 = 0
  [evaluate] log-likelihood = -1533.27
  [evaluate]    chi2_mflike.MFLike = 3066.53
  [evaluate] Derived params:
  [evaluate]    As = 2.0989e-09
#+end_example

Finally, we can directly catch the value returned by =logp= function from =MFLike= likelihood
#+BEGIN_SRC jupyter-python
  logp = mflike.logp(**nuisance_params)
  print("log-likelihood value =", logp)
  print("Χ² value =", -2*(logp-mflike.logp_const))
#+END_SRC

#+RESULTS:
: log-likelihood value = -1533.2663524458883
: Χ² value = 2420.688324179856

* Plotting CMB power spectra

Here we get $C_\ell$ for different mode and a different range of [\ell_{min};\ell_{max}] than the
default range of =MFLike=. Make sure to call the =model.logposterior= line with the =cached=False=
option in order to recompute \(C_\ell\)s given the new \ell range.
#+BEGIN_SRC jupyter-python :results none
  lmin, lmax = 2, 9000
  Cl = {"tt": lmax, "ee": lmax, "te": lmax, "bb":lmax}
  model.theory["camb"].needs(Cl=Cl)
  model.logposterior({})
  Dls = model.theory["camb"].get_Cl(ell_factor=True)
#+END_SRC

Let's plot the different spectra
#+BEGIN_SRC jupyter-python
  ell = np.arange(lmin, lmax)
  dls = {cl: Dls[cl][lmin:lmax] for cl in Cl.keys()}
  fig, axes = plt.subplots(2, 1, sharex=True, figsize=(6, 8))
  axes[0].set_yscale("log")
  for i, cl in enumerate(Cl.keys()):
      ax = axes[1] if cl == "te" else axes[0]
      ax.plot(ell, dls[cl], "-C{}".format(i), label=cl.upper())

  for ax in axes:
      ax.set_ylabel(r"$D_\ell$")
      ax.legend()
      axes[1].set_xlabel(r"$\ell$")
      plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/7239c97f2cce66687a394fff72e2e37b56cc3cf5.png]]

* Plotting foregrounds

Get all the foreground models at higher \ell
#+BEGIN_SRC jupyter-python :results none
  from mflike import get_foreground_model
  fg_models = get_foreground_model(nuisance_params, mflike.foregrounds, mflike.freqs, ell=ell)
#+END_SRC

Then plot them in a triangle plot
#+BEGIN_SRC jupyter-python
  mode = "tt"
  components = mflike.foregrounds["components"][mode]
  freqs = sorted(mflike.freqs)
  nfreqs = len(freqs)
  fig, axes = plt.subplots(nfreqs, nfreqs, sharex=True, sharey=True, figsize=(10, 10))
  from itertools import product
  for i, cross in enumerate(product(freqs, freqs)):
      idx = (i%nfreqs, i//nfreqs)
      ax = axes[idx]
      if idx in zip(*np.triu_indices(nfreqs, k=1)):
        fig.delaxes(ax)
        continue
      ax.plot(ell, fg_models[mode, "all", cross[0], cross[1]], color="k")
      for compo in components:
        ax.plot(ell, fg_models[mode, compo, cross[0], cross[1]])
      ax.plot(ell, dls[mode], color="tab:gray")
      ax.legend([], title="{}x{} GHz".format(*cross))
      if mode == "tt":
        ax.set_yscale("log")
        ax.set_ylim(10**-1, 10**4)

  for i in range(nfreqs):
    axes[-1, i].set_xlabel("$\ell$")
    axes[i, 0].set_ylabel("$D_\ell$")
  fig.legend(["all"] + components, title=mode.upper(), bbox_to_anchor=(0.5, 1))
  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/a37a498c5355b379342fbb845869424ae8c56632.png]]

* Plotting simulations & residuals
Simulations and covariance are stored as =sacc= objects. Here we will read the power spectra for all
the simulations and compare its mean value to theory + foregrounds. For more details regarding =sacc=
format, you can have a look to the tutorial [[https://github.com/simonsobs/sacc/tree/master/examples][notebooks]] or you can refer to the official [[https://sacc.readthedocs.io/en/latest/][documentation]]

Here we define function utilities related to =sacc= files
#+BEGIN_SRC jupyter-python :results none
  import os
  import sacc
  def _get_tracers(spec, cross):
      spins = {"t": ("0", "s0"), "e": ("e", "s2"), "b": ("b", "s2")}
      data_type = "cl_{}{}".format(spins[spec[0]][0], spins[spec[1]][0])
      tracers = ["LAT_{}_{}".format(cross[i], spins[spec[i]][1]) for i in range(2)]
      return data_type, *tracers

  def read_sacc_data(spec, cross, sim_id):
      fn = os.path.join(mflike.data_folder, "data_sacc_{:05d}.fits".format(sim_id))
      s = sacc.Sacc.load_fits(fn)
      return s.get_ell_cl(*_get_tracers(spec, cross))

  fn = os.path.join(mflike.data_folder, "data_sacc_w_covar_and_Bbl.fits")
  s_b = sacc.Sacc.load_fits(fn)
  def read_sacc_cov(spec, cross):
      return s_b.get_ell_cl(*_get_tracers(spec, cross), return_cov=True, return_windows=True)[-2:]
#+END_SRC

We can plot unbinned theory + foreground model with a bunch of simulated files
#+BEGIN_SRC jupyter-python
  cross = (93, 93)
  nsims = 10

  fig, axes = plt.subplots(2, 2, sharex=True, figsize=(10, 8))
  for spec, ax in zip(Cl.keys(), axes.flatten()):
      cls = []
      for i in range(nsims):
          cls += [read_sacc_data(spec, cross, i)]
          ax.plot(*cls[-1], color="tab:gray")
      ax.plot(ell, dls[spec], color="tab:red", label="theory")
      fg = fg_models[spec, "all", cross[0], cross[1]] if spec != "bb" else np.zeros_like(ell)
      ax.plot(ell, fg, color="tab:blue", label="all foregrounds")
      ax.plot(*np.mean(cls, axis=0), "--k", label="mean sim.")
      ax.legend(title="{} - {}x{} GHz".format(spec.upper(), *cross))
      if spec == "tt":
        ax.set_yscale("log")
        ax.set_ylim(10**-1, 10**4)

  for ax in axes[:, 0]:
      ax.set_ylabel(r"$D_\ell$")
  for ax in axes[-1]:
      ax.set_xlabel(r"$\ell$")
  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/66677cfd4a08e2caa1d93496175778acd6a6cc50.png]]

We can also plot the residuals /i.e./ simulations - (theory + foregrounds) for the different spectra
and cross frequencies (we have to take care of the \ell range of Bbl and cut power spectra
consequently)
#+BEGIN_SRC jupyter-python
  spectra = ["tt", "ee", "te", "et"]
  isim = 0

  from itertools import combinations_with_replacement as cwr
  crosses = list(cwr(freqs, 2))
  fig, axes = plt.subplots(len(crosses), 4, sharex=True, figsize=(16, 2*len(crosses)))
  for i, cross in enumerate(crosses):
      for j, spec in enumerate(spectra):
          ax = axes[i, j]
          try:
              cov, bbl = read_sacc_cov(spec, cross)
              inv_cov = np.linalg.inv(cov)
          except:
              # No cov for "ET" and same frequencies
              fig.delaxes(ax)
              continue
          l, cl = read_sacc_data(spec, cross, isim)
          ps_th = dls[spec if spec != "et" else "te"]
          ps_fg = fg_models[spec if spec != "et" else "te", "all", cross[0], cross[1]]
          cl_model = np.dot(bbl[1], ps_th[:6000]+ps_fg[:6000])
          delta_cl = cl - cl_model
          cl_err = np.sqrt(np.diag(cov))
          if spec == "tt":
              ax.errorbar(l, l**2*delta_cl, l**2*cl_err, fmt=".C{}".format(j))
          else:
              ax.errorbar(l, delta_cl, cl_err, fmt=".C{}".format(j))
          chi2 = delta_cl @ inv_cov @ delta_cl
          title = "%s - %dx%d GHz\n$\chi2$/ndf = %.2f" % (spec.upper(), *cross, chi2/len(delta_cl))
          axes[i, j].legend([], title=title)

  for ax in axes[:, 0]:
      ax.set_ylabel(r"$\ell^2\Delta D_\ell$")
  for ax in axes[:, 1:].flatten():
      ax.set_ylabel(r"$\Delta D_\ell$")
  for ax in axes[-1]:
      ax.set_xlabel(r"$\ell$")
  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/ad920d8ce9b112773363de6aff41c76a6d9c59e8.png]]

#+BEGIN_SRC jupyter-python
  fig, axes = plt.subplots(len(crosses), 4, sharex="col", sharey="row",
                           figsize=(8, 12))
  for ax in axes.flatten():
    ax.set_xticks([])
    ax.set_yticks([])
  plt.subplots_adjust(hspace=0.1, wspace=0.1)
  for i, cross in enumerate(crosses):
      for j, spec in enumerate(spectra):
          ax = axes[i, j]
          try:
              cov, bbl = read_sacc_cov(spec, cross)
              inv_cov = np.linalg.inv(cov)
              diag = np.sqrt(np.diag(inv_cov))
              norm = np.outer(diag, diag)
          except:
              # No cov for "ET" and same frequencies
              fig.delaxes(ax)
              continue
          ax.imshow(np.log10(np.abs(inv_cov)), cmap="RdBu")
          ax.legend([], title="{}x{} GHz".format(*cross))
          # ax.imshow(inv_cov/norm, cmap="RdBu")
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/fe7cb481410bdda60eb72debb317fb24061fc015.png]]

* Fisher matrix

Even if we do not need priors to compute Fisher matrix, we need to fool =cobaya= in order to change
parameter values. We set the prior values to \pm 10% arround the central value. We need to take care
of =logA= parameter since =cobaya= will sample it but theory module will use the derivative =As= value.
#+BEGIN_SRC jupyter-python :results none
  sampled_params = {**cosmo_params, **nuisance_params}.copy()
  sampled_params.update({k: {"prior": {"min": 0.9*v, "max": 1.1*v}}
                         for k, v in sampled_params.items() if k not in ["logA", "As"]})
  sampled_params["logA"] = {"prior": {"min": 0.9*cosmo_params["logA"]["value"],
                                      "max": 1.1*cosmo_params["logA"]["value"]},
                            "drop": True}
#+END_SRC

Then we define a new model (after having close the previous one to release memory allocation) and
get the =MFLike= likelihood
#+BEGIN_SRC jupyter-python
  model.close()
  info = {
      "params": sampled_params,
      "likelihood": mflike_config,
      "theory": {"camb": {"extra_args": {"lens_potential_accuracy": 1}}},
      "modules": "/tmp/modules"
  }
  from cobaya.model import get_model
  model = get_model(info)
  mflike = model.likelihood["mflike.MFLike"]
#+END_SRC

#+RESULTS:
: [camb] Importing *local* CAMB from /tmp/modules/code/CAMB
: [mflike.mflike] Initialising.

Given the sampled parameters, we now set the defaults value of parameters in the same order as the
=cobaya='s one
#+BEGIN_SRC jupyter-python :results none
  default_values = {**cosmo_params, **nuisance_params}.copy()
  default_values["logA"] = cosmo_params["logA"]["value"]
  defaults = {k: default_values[k] for k in model.parameterization.sampled_params().keys()}
#+END_SRC

and we define the list of Fisher parameters
#+BEGIN_SRC jupyter-python :results none
  fisher_params = list(defaults.keys())
  for p in ["tau", "n_CIBC", "T_d"]:
      fisher_params.remove(p)
#+END_SRC

For each parameter, we will compute the associated power spectra by slightly modifying the central
value of the parameter (\pm\epsilon). The power spectra is taken from =mflike._get_power_spectra=
given the nuisance parameters and we also need to recompute (if necessary) the theoritical
\(C_\ell\)s. The Fisher algorithm is then
#+BEGIN_SRC jupyter-python :results none
  deriv = {k: None for k in fisher_params}
  for i, p in enumerate(fisher_params):
      def _get_power_spectra(epsilon):
          point = defaults.copy()
          point.update({p: point[p]*(1+epsilon)})
          model.logposterior(point)
          cl = model.theory["camb"].get_Cl(ell_factor=True)
          return mflike._get_power_spectra(cl, **point)
      epsilon = 0.01
      ps_minus = _get_power_spectra(-epsilon)
      ps_plus = _get_power_spectra(+epsilon)
      delta = (ps_plus - ps_minus)/(2*epsilon*defaults[p])
      if np.all(delta == 0):
          print("WARNING: Sampling a parameter '{}' that do not have "
                "any effect on power spectra! You should remove it from "
                "cobaya parameter dictionary.".format(p))
          fisher_params.remove(p)
          continue
      deriv[p] = delta

  nparams = len(fisher_params)
  fisher_matrix = np.empty((nparams, nparams))
  for i1, p1 in enumerate(fisher_params):
      for i2, p2 in enumerate(fisher_params):
          fisher_matrix[i1, i2] = np.dot(deriv[p1], mflike.inv_cov.dot(deriv[p2]))
  fisher_cov = np.linalg.inv(fisher_matrix)
#+END_SRC

We can show the correlation matrix between parameters
#+BEGIN_SRC jupyter-python
  fisher_sigmas = np.sqrt(np.diag(fisher_cov))
  norm = np.outer(fisher_sigmas, fisher_sigmas)
  fisher_corr = fisher_cov / norm
  plt.figure(figsize=(6, 6))
  ind = np.triu_indices_from(fisher_corr)
  fisher_corr[ind] = np.nan
  plt.imshow(fisher_corr, cmap="RdBu")
  plt.xticks(np.arange(nparams), fisher_params, rotation=90)
  plt.yticks(np.arange(nparams), fisher_params, rotation=0);
  cbar = plt.colorbar(shrink=0.8)
  cbar.set_label("correlation coefficient")
  [s.set_visible(False) for s in plt.gca().spines.values()];
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/4fc854374514b8e72f0e43387665357c594ed6eb.png]]

and the Fisher estimated noise
#+BEGIN_SRC jupyter-python
  for param_name, param_sigma in zip(fisher_params, fisher_sigmas):
      print("param: {}, sigma: {}, Fisher S/N: {}".format(
          param_name, param_sigma, defaults[param_name]/param_sigma))
#+END_SRC

#+RESULTS:
#+begin_example
  param: cosmomc_theta, sigma: 1.3838067138167586e-06, Fisher S/N: 7521.642940502655
  param: logA, sigma: 0.003412296072219381, Fisher S/N: 892.0679611544263
  param: ombh2, sigma: 6.223475637959309e-05, Fisher S/N: 359.4454497990961
  param: omch2, sigma: 0.001077572842962918, Fisher S/N: 111.3613810738264
  param: ns, sigma: 0.003069207497114361, Fisher S/N: 314.3808298745489
  param: Alens, sigma: 0.020999158635721667, Fisher S/N: 47.62095555099527
  param: a_tSZ, sigma: 0.038838541839749496, Fisher S/N: 84.96714458580931
  param: a_kSZ, sigma: 0.08898556368181341, Fisher S/N: 17.980444622693366
  param: a_p, sigma: 0.06981708936840139, Fisher S/N: 98.82967139450646
  param: beta_p, sigma: 0.01308316774481468, Fisher S/N: 158.9829038784875
  param: a_c, sigma: 0.10284267941619281, Fisher S/N: 47.64558865848146
  param: beta_c, sigma: 0.027226926236267917, Fisher S/N: 80.80236384044949
  param: a_s, sigma: 0.010756481630169578, Fisher S/N: 288.19832604977245
#+end_example

Let's show the Signal over Noise ratio graphically
#+BEGIN_SRC jupyter-python
  plt.figure(figsize=(7, 6))
  SoN = [defaults[param_name]/param_sigma
         for param_name, param_sigma in zip(fisher_params, fisher_sigmas)]
  colors = ["tab:blue" if name in cosmo_params else "tab:purple" for name in fisher_params]
  plt.barh(np.arange(len(SoN)), SoN, color=colors, alpha=0.8)
  plt.xscale("log")
  plt.xlabel("S/N")
  plt.yticks(range(len(fisher_params)), fisher_params)
  plt.gca().spines["right"].set_color(None)
  plt.gca().spines["top"].set_color(None)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/cbc5e629a2a04c32ae379864624c87965bc0aa70.png]]

It also works for TT, TE or EE mode even if you keep the default list of sampled parameters. It will
only warn you about the fact that some parameters have no effect on power spectra and thus can be
removed from the sampled parameter list.

* Using the MFLike likelihood without cobaya

The =MFLike= likelihood can also be used independently of =cobaya=. The principle is the same as in
this =cobaya='s [[https://github.com/CobayaSampler/planck_lensing_external][example]]. First we need
to instantiate an =MFLike= object
#+BEGIN_SRC jupyter-python
  from mflike import MFLike
  my_mflike = MFLike({"path_install": r"/tmp/modules",
                      "input_file": "data_sacc_00000.fits",
                      "cov_Bbl_file": "data_sacc_w_covar_and_Bbl.fits"
  })
#+END_SRC

#+RESULTS:
: [mflike.mflike] Initialising.

To compute the log-likelihood value, we can use the =loglike= function which takes as parameters an
theoritical $C_\ell$ dictionary and the nuisance parameter values. The $C_\ell$ dictionary can be,
for instance, retrieved from an independant program or an independant computation. Here we will use
=CAMB= to compute the $C_\ell$ from a cosmological model set by the =cosmo_params=

#+BEGIN_SRC jupyter-python :results none
  camb_cosmo = {k: v for k, v in cosmo_params.items()
                if k not in ["logA", "As"]}
  camb_cosmo.update({"As": 1e-10*np.exp(cosmo_params["logA"]["value"]),
                     "lmax": lmax, "lens_potential_accuracy": 1})

  pars = camb.set_params(**camb_cosmo)
  results = camb.get_results(pars)
  powers = results.get_cmb_power_spectra(pars, CMB_unit="muK")
  cl_dict = {k: powers["total"][:, v]
             for k, v in {"tt": 0, "ee": 1, "te": 3, "bb": 2}.items()}
#+END_SRC

Let's plot $C_\ell$ and let's compare them to the ones from =cobaya=
#+BEGIN_SRC jupyter-python
  fig = plt.figure(figsize=(8, 6))
  gs = plt.GridSpec(4, 2)
  axes = {"tt": [fig.add_subplot(gs[0:3, 0], xticklabels=[]), fig.add_subplot(gs[-1, 0])],
          "te": [fig.add_subplot(gs[0:3, 1], xticklabels=[]), fig.add_subplot(gs[-1, 1])]}

  l = np.arange(cl_dict["tt"].shape[0])
  for i, (k, v) in enumerate(cl_dict.items()):
      plot = axes["te"][0].plot if k == "te" else axes["tt"][0].semilogy
      plot(l, v, "-C{}".format(i), label=k.upper())
      plot(l, Dls[k], "--C{}".format(i))
      ax = axes["te"][1] if k == "te" else axes["tt"][1]
      ax.plot(l, v - Dls[k], "-C{}".format(i))

  axes["tt"][0].set_ylabel(r"$D_\ell$")
  axes["tt"][1].set_ylabel(r"$\Delta D_\ell$")
  for k in axes.keys():
      axes[k][1].set_xlabel(r"$\ell$")
      axes[k][0].plot([], [], "-k", label="CAMB")
      axes[k][0].plot([], [], "--k", label="cobaya")
      axes[k][0].legend(ncol=1 if k == "te" else 2)
  plt.tight_layout()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/e27794dde420059d84c1b2e0623f34f0717f0f30.png]]

Now we can inject these $C_\ell$ into the =loglike= function to get the corresponding log-likelihood
value
#+BEGIN_SRC jupyter-python
  loglike = my_mflike.loglike(cl_dict, **nuisance_params)
  print("log-likelihood value =", loglike)
  print("Χ² value =", -2*(loglike-mflike.logp_const))
#+END_SRC

#+RESULTS:
: log-likelihood value = -1533.2663524458226
: Χ² value = 2420.688324179726
